{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Streaming application using Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
    "import time\n",
    "import shutil\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Spark and PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, FloatType, \n",
    "    TimestampType, BooleanType, DoubleType\n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, count, sum, min, max, mean, hour, expr, udf\n",
    ")\n",
    "\n",
    "# Spark and PySpark imports\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, FloatType, \n",
    "    TimestampType, BooleanType, DoubleType\n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, count, sum, min, max, mean, hour, expr, udf\n",
    ")\n",
    "from pyspark.sql.functions import from_unixtime, col\n",
    "from pyspark.sql.types import IntegerType, TimestampType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql import functions as F\n",
    "# PySpark MLlib imports\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\tWrite code to create a SparkSession, which 1) uses four cores with a proper application name; 2) use the Melbourne timezone; 3) ensure a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define the checkpoint location\n",
    "checkpoint_location = \"../A2B\"\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Assignment 2B part 2\")\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"Australia/Melbourne\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Set the checkpoint directory\n",
    "spark.sparkContext.setCheckpointDir(checkpoint_location)\n",
    "\n",
    "# Example usage\n",
    "print(\"Spark Session created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tSimilar to assignment 2A, write code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. customer, product, category) into data frames. (You can use your code from 2A.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the customer dataset\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", FloatType(), False),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"birthdate\", TimestampType(), True),\n",
    "    StructField(\"first_join_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Define the schema for the category dataset\n",
    "category_schema = StructType([\n",
    "    StructField(\"category_id\", FloatType(), False),\n",
    "    StructField(\"cat_level1\", StringType(), True),\n",
    "    StructField(\"cat_level2\", StringType(), True),\n",
    "    StructField(\"cat_level3\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the schema for the browsing behaviour dataset\n",
    "browsing_behaviour_schema = StructType([\n",
    "    StructField(\"session_id\", StringType(), False),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"traffic_source\", StringType(), True),\n",
    "    StructField(\"device_type\", StringType(), True),\n",
    "    StructField(\"ts\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Define the schema for the product dataset\n",
    "product_schema = StructType([\n",
    "    StructField(\"id\", StringType(), False),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"baseColour\", StringType(), True),\n",
    "    StructField(\"season\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"usage\", StringType(), True),\n",
    "    StructField(\"productDisplayName\", StringType(), True),\n",
    "    StructField(\"category_id\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Product metadata schema\n",
    "product_metadata_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"product_id\", IntegerType()),\n",
    "        StructField(\"quantity\", IntegerType()),\n",
    "        StructField(\"item_price\", IntegerType())\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define the schema for the customer session dataset\n",
    "customer_session_schema = StructType([\n",
    "    StructField(\"session_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", FloatType(), False)  \n",
    "])\n",
    "\n",
    "# Define the schema for the fraud transaction dataset\n",
    "fraud_transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"is_fraud\", BooleanType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage to create DataFrames with the defined schemas\n",
    "df_category = spark.read.csv(\"category.csv\", schema=category_schema, header = True)\n",
    "df_customer = spark.read.csv(\"customer.csv\", schema=customer_schema, header = True)\n",
    "df_product = spark.read.csv(\"product.csv\", schema=product_schema, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using the Kafka topics from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'ts' column, you shall receive it as an Int type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the logs data stream for new log data\n",
    "trans_topic_name = 'trans_topic'\n",
    "browsing_topic_name = 'browsing_behaviour_topic'\n",
    "\n",
    "# Read the stream from Kafka\n",
    "# Browsing\n",
    "df_browsing_urls = spark.readStream.format(\"kafka\")\\\n",
    "                        .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\\\n",
    "                        .option(\"subscribe\", browsing_topic_name)\\\n",
    "                        .load()\n",
    "\n",
    "\n",
    "# Get value of the kafka message\n",
    "browsing_lines = df_browsing_urls.selectExpr(\"CAST(value AS STRING)\")\n",
    "# Apply the schema to the DataFrame by parsing the JSON\n",
    "df_browsing_behaviour = browsing_lines.select(\n",
    "    from_json(col(\"value\"), browsing_behaviour_schema).alias(\"browsing_value\")\n",
    ").select(\"browsing_value.*\")  # Expand the struct to get individual fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- ts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_browsing_behaviour.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+--------------+-----------+---+\n",
      "|session_id|event_type|event_time|traffic_source|device_type|ts |\n",
      "+----------+----------+----------+--------------+-----------+---+\n",
      "+----------+----------+----------+--------------+-----------+---+\n",
      "\n",
      "+------------------------------------+----------+-----------------------+--------------+-----------+----------+\n",
      "|session_id                          |event_type|event_time             |traffic_source|device_type|ts        |\n",
      "+------------------------------------+----------+-----------------------+--------------+-----------+----------+\n",
      "|6691e6b4-6558-4cd1-b3e3-2b078edf5919|SCR       |2024-05-09 10:01:43.864|MOBILE        |Android    |1728818397|\n",
      "|e441523d-4464-4e52-9a4e-86c07c877332|CO        |2024-05-09 10:01:50.943|MOBILE        |Android    |1728818397|\n",
      "|a42e2c54-c770-49fd-8754-bed1eeea235c|VP        |2024-05-09 10:01:54.151|MOBILE        |Android    |1728818397|\n",
      "|b4182d6b-d547-4585-a275-49495792b24a|CO        |2024-05-09 10:01:55.35 |MOBILE        |Android    |1728818397|\n",
      "|2647f232-1be8-48c5-8abf-ba9503fa9f68|ATC       |2024-05-09 10:02:00.993|MOBILE        |Android    |1728818397|\n",
      "+------------------------------------+----------+-----------------------+--------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------------------------+----------+-----------------------+--------------+-----------+----------+\n",
      "|session_id                          |event_type|event_time             |traffic_source|device_type|ts        |\n",
      "+------------------------------------+----------+-----------------------+--------------+-----------+----------+\n",
      "|34b2db0a-e9d8-4a0d-9fc4-8003be109f7f|CL        |2024-05-09 12:50:39.129|MOBILE        |Android    |1728818400|\n",
      "|5e4be3fd-a783-4cce-9757-6f4506bbdf12|ATC       |2024-05-09 12:50:43.39 |MOBILE        |iOS        |1728818400|\n",
      "|ccbaf34a-e6a0-45bf-a58b-56750a577a14|VP        |2024-05-09 12:50:46.141|MOBILE        |Android    |1728818400|\n",
      "|74adfdbb-90de-4a9b-9a8c-a72fdf3c0894|HP        |2024-05-09 12:50:50.676|MOBILE        |iOS        |1728818400|\n",
      "|5dc22240-0b6c-4b92-b509-1efb1e1d13b2|SCR       |2024-05-09 12:50:53.184|MOBILE        |Android    |1728818400|\n",
      "+------------------------------------+----------+-----------------------+--------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the stream for browsing behaviour\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.show(5,False)\n",
    "df_browsing_behaviour_query = df_browsing_behaviour.writeStream.outputMode(\"append\")\\\n",
    "        .foreachBatch(foreach_batch_function)\\\n",
    "        .trigger(processingTime='5 seconds')\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_browsing_behaviour_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transaction\n",
    "df_trans_urls = spark.readStream.format(\"kafka\")\\\n",
    "                        .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\\\n",
    "                        .option(\"subscribe\", trans_topic_name)\\\n",
    "                        .load()\n",
    "# Optional: Display the schema of the DataFrame\n",
    "df_trans_urls.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_extract\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Cast the Kafka key and value as string\n",
    "df_kafka_values = df_trans_urls.select(col(\"key\").cast(\"string\"), col(\"value\").cast(\"string\"))\n",
    "\n",
    "# Apply regex to extract each field from the value (assumed to be JSON-like)\n",
    "df_transactions = df_kafka_values.withColumn(\"created_at\", regexp_extract(col(\"value\"), '\"created_at\":\\s*\"([^\"]+)\"', 1))\\\n",
    "    .withColumn(\"customer_id\", regexp_extract(col(\"value\"), '\"customer_id\":\\s*\"([^\"]+)\"', 1))\\\n",
    "    .withColumn(\"transaction_id\", regexp_extract(col(\"value\"), '\"transaction_id\":\\s*\"([^\"]+)\"', 1))\\\n",
    "    .withColumn(\"session_id\", regexp_extract(col(\"value\"), '\"session_id\":\\s*\"([^\"]+)\"', 1))\\\n",
    "    .withColumn(\"product_metadata\", regexp_extract(col(\"value\"), '\"product_metadata\":\\s*\"\\[([^\"]+)\\]\"', 1))\\\n",
    "    .withColumn(\"payment_method\", regexp_extract(col(\"value\"), '\"payment_method\":\\s*\"([^\"]+)\"', 1))\\\n",
    "    .withColumn(\"payment_status\", regexp_extract(col(\"value\"), '\"payment_status\":\\s*\"([^\"]+)\"', 1))\\\n",
    "    .withColumn(\"promo_amount\", regexp_extract(col(\"value\"), '\"promo_amount\":\\s*\"([^\"]+)\"', 1))\\\n",
    "    .withColumn(\"promo_code\", regexp_extract(col(\"value\"), '\"promo_code\":\\s*\"([^\"]+)\"', 1))\\\n",
    "    .withColumn(\"shipment_fee\", regexp_extract(col(\"value\"), '\"shipment_fee\":\\s*\"([^\"]+)\"', 1))\\\n",
    "    .withColumn(\"shipment_location_lat\", regexp_extract(col(\"value\"), '\"shipment_location_lat\":\\s*\"([^\"]+)\"', 1))\\\n",
    "    .withColumn(\"shipment_location_long\", regexp_extract(col(\"value\"), '\"shipment_location_long\":\\s*\"([^\"]+)\"', 1))\\\n",
    "    .withColumn(\"total_amount\", regexp_extract(col(\"value\"), '\"total_amount\":\\s*\"([^\"]+)\"', 1))\\\n",
    "    .withColumn(\"clear_payment\", regexp_extract(col(\"value\"), '\"clear_payment\":\\s*\"([^\"]+)\"', 1))\\\n",
    "    .withColumn(\"trans_ts\", regexp_extract(col(\"value\"), '\"trans_ts\":\\s*([0-9]+)', 1))\n",
    "\n",
    "# Select the key and extracted fields\n",
    "df_transactions = df_transactions.select(\n",
    "    col(\"created_at\"),\n",
    "    col(\"customer_id\"),\n",
    "    col(\"transaction_id\"),\n",
    "    col(\"session_id\"),\n",
    "    col(\"product_metadata\"),\n",
    "    col(\"payment_method\"),\n",
    "    col(\"payment_status\"),\n",
    "    col(\"promo_amount\"),\n",
    "    col(\"promo_code\"),\n",
    "    col(\"shipment_fee\"),\n",
    "    col(\"shipment_location_lat\"),\n",
    "    col(\"shipment_location_long\"),\n",
    "    col(\"total_amount\"),\n",
    "    col(\"clear_payment\"),\n",
    "    col(\"trans_ts\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------+----------+----------------+--------------+--------------+------------+----------+------------+---------------------+----------------------+------------+-------------+--------+\n",
      "|created_at|customer_id|transaction_id|session_id|product_metadata|payment_method|payment_status|promo_amount|promo_code|shipment_fee|shipment_location_lat|shipment_location_long|total_amount|clear_payment|trans_ts|\n",
      "+----------+-----------+--------------+----------+----------------+--------------+--------------+------------+----------+------------+---------------------+----------------------+------------+-------------+--------+\n",
      "+----------+-----------+--------------+----------+----------------+--------------+--------------+------------+----------+------------+---------------------+----------------------+------------+-------------+--------+\n",
      "\n",
      "+-----------------------+-----------+------------------------------------+------------------------------------+----------------------------------------------------------------------------------------------------------------------+--------------+--------------+------------+----------+------------+---------------------+----------------------+------------+-------------+----------+\n",
      "|created_at             |customer_id|transaction_id                      |session_id                          |product_metadata                                                                                                      |payment_method|payment_status|promo_amount|promo_code|shipment_fee|shipment_location_lat|shipment_location_long|total_amount|clear_payment|trans_ts  |\n",
      "+-----------------------+-----------+------------------------------------+------------------------------------+----------------------------------------------------------------------------------------------------------------------+--------------+--------------+------------+----------+------------+---------------------+----------------------+------------+-------------+----------+\n",
      "|2024-05-09 23:43:58.683|61734      |6c84433d-4956-47cf-b3af-1742deb98a7b|d6a55f06-2263-40a4-bc8d-7419412deda5|{'product_id': 28999, 'quantity': 1, 'item_price': 184929}                                                            |Debit Card    |Success       |10547       |STARTUP   |10000       |-7.84743331953587    |110.314240469082      |184382      |1            |1728818414|\n",
      "|2024-05-09 23:44:01.821|49396      |63cd703a-f8d8-490f-9e9f-2fc39647b506|20de0168-6321-456b-a9f6-05a5c14ab98c|{'product_id': 58296, 'quantity': 1, 'item_price': 278120}                                                            |OVO           |Fail          |0           |          |0           |-2.70341487816083    |114.80534003096       |278120      |0            |1728818414|\n",
      "|2024-05-09 23:44:02.743|73942      |281521c4-42bb-4656-882c-36b234f968b9|7c55891f-7693-4b49-9e1c-a00d8dfdebfa|{'product_id': 53921, 'quantity': 1, 'item_price': 276840}                                                            |OVO           |Fail          |0           |          |50000       |-6.23921265551327    |106.875238505709      |326840      |0            |1728818414|\n",
      "|2024-05-09 23:46:08.520|97280      |a53cc5ff-fd35-4d9a-a76e-ecc0d761af7a|95913265-325d-4dae-a315-073375d8f9c8|{'product_id': 38375, 'quantity': 4, 'item_price': 145843}, {'product_id': 55805, 'quantity': 1, 'item_price': 332825}|OVO           |Success       |3506        |XX2022    |10000       |-7.06560375098552    |107.900203819376      |922691      |1            |1728818414|\n",
      "|2024-05-09 23:47:11.257|76664      |d75796d0-c3f0-45ef-b4db-3944a6686103|42ebce2a-3ade-4da3-952a-14a6ac363eab|{'product_id': 31658, 'quantity': 1, 'item_price': 398555}                                                            |Debit Card    |Success       |6902        |AZ2022    |0           |-6.91346632968492    |108.887175837431      |391653      |1            |1728818414|\n",
      "+-----------------------+-----------+------------------------------------+------------------------------------+----------------------------------------------------------------------------------------------------------------------+--------------+--------------+------------+----------+------------+---------------------+----------------------+------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the stream for transaction\n",
    "query_trans = df_transactions.writeStream.outputMode(\"append\")\\\n",
    "        .foreachBatch(foreach_batch_function)\\\n",
    "        .trigger(processingTime='5 seconds')\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_trans.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\tThen, the streaming data format should be transformed into the proper formats following the metadata file schema, similar to assignment 2A. Perform the following tasks:  \n",
    "a)\tFor the 'ts' column, convert it to the timestamp format, we will use it as event_ts.  \n",
    "b)\tIf the data is late for more than 2 minutes, discard it.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- event_ts: timestamp (nullable = true)\n",
      " |-- event_hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get event_ts from ts columns with datatype format\n",
    "df_browsing_behaviour_watermark = df_browsing_behaviour.withColumn(\"event_ts\", F.from_unixtime(F.col('ts').cast(IntegerType())).cast(TimestampType())) \\\n",
    "    .drop(\"ts\")  # Drop the original 'ts' column if not needed\n",
    "\n",
    "df_browsing_behaviour_watermark = df_browsing_behaviour_watermark.withColumn(\"event_hour\", hour(col(\"event_time\")))\n",
    "\n",
    "\n",
    "df_browsing_behaviour_watermark = df_browsing_behaviour_watermark.withWatermark('event_ts', \"2 minutes\")\n",
    "df_browsing_behaviour_watermark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+----------+-----------------------+--------------+-----------+-------------------+----------+\n",
      "|session_id                          |event_type|event_time             |traffic_source|device_type|event_ts           |event_hour|\n",
      "+------------------------------------+----------+-----------------------+--------------+-----------+-------------------+----------+\n",
      "|fd702ad0-fb4b-426c-b5cf-374b94e03561|HP        |2024-05-10 05:02:38.522|WEB           |Android    |2024-10-13 22:20:19|5         |\n",
      "|e552c3db-5dfb-4743-b84c-b1043fbcb283|HP        |2024-05-10 05:02:40.27 |MOBILE        |iOS        |2024-10-13 22:20:19|5         |\n",
      "|b2679c57-2a0b-4b5e-b0b5-af885a8cefb6|ATC       |2024-05-10 05:02:48.239|MOBILE        |iOS        |2024-10-13 22:20:19|5         |\n",
      "|400997c3-0dfb-435a-a4f1-55e3e82dcc49|HP        |2024-05-10 05:02:48.38 |MOBILE        |iOS        |2024-10-13 22:20:19|5         |\n",
      "|8dd00e4f-57e8-4b99-b94c-6b737fcb90a8|HP        |2024-05-10 05:02:57.122|MOBILE        |Android    |2024-10-13 22:20:19|5         |\n",
      "+------------------------------------+----------+-----------------------+--------------+-----------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------------------------+----------+-----------------------+--------------+-----------+-------------------+----------+\n",
      "|session_id                          |event_type|event_time             |traffic_source|device_type|event_ts           |event_hour|\n",
      "+------------------------------------+----------+-----------------------+--------------+-----------+-------------------+----------+\n",
      "|1df9d20a-702c-4583-9280-cbecadd37ae9|ATC       |2024-05-10 06:00:21.114|MOBILE        |Android    |2024-10-13 22:20:22|6         |\n",
      "|bc241259-a13d-46f4-bc07-83e324266131|SER       |2024-05-10 06:00:22.864|MOBILE        |iOS        |2024-10-13 22:20:22|6         |\n",
      "|85ff6256-2f5b-41ea-9dc2-387b6674e5d2|VI        |2024-05-10 06:00:24.19 |MOBILE        |Android    |2024-10-13 22:20:22|6         |\n",
      "|b478c44a-f94a-4acc-a5f7-8a61010da5f2|HP        |2024-05-10 06:00:26.484|WEB           |Android    |2024-10-13 22:20:22|6         |\n",
      "|6c92bfbf-58c5-452e-9721-89b6f5efc0e0|ATC       |2024-05-10 06:00:28.164|MOBILE        |Android    |2024-10-13 22:20:22|6         |\n",
      "+------------------------------------+----------+-----------------------+--------------+-----------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the stream for behaviour watermark streamline\n",
    "df_browsing_behaviour_watermark_query = df_browsing_behaviour_watermark \\\n",
    "    .writeStream.outputMode(\"append\") \\\n",
    "    .foreachBatch(foreach_batch_function) \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_browsing_behaviour_watermark_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data type for transactions\n",
    "df_transactions = df_transactions \\\n",
    "    .withColumn('created_at', F.col('created_at').cast(TimestampType())) \\\n",
    "    .withColumn('customer_id', F.col('customer_id').cast(IntegerType())) \\\n",
    "    .withColumn('promo_amount', F.col('promo_amount').cast(DoubleType())) \\\n",
    "    .withColumn('shipment_fee', F.col('shipment_fee').cast(DoubleType())) \\\n",
    "    .withColumn('shipment_location_lat', F.col('shipment_location_lat').cast(DoubleType())) \\\n",
    "    .withColumn('shipment_location_long', F.col('shipment_location_long').cast(DoubleType())) \\\n",
    "    .withColumn('total_amount', F.col('total_amount').cast(IntegerType())) \\\n",
    "    .withColumn('clear_payment', F.col('clear_payment').cast(IntegerType())) \\\n",
    "    .withColumn(\n",
    "        \"product_metadata\",\n",
    "        from_json(col(\"product_metadata\"), product_metadata_schema)\n",
    "    )\\\n",
    "    .withColumn('trans_ts', F.col('trans_ts').cast(IntegerType())) \\\n",
    "    .withColumn(\"event_trans_ts\",from_unixtime(col('trans_ts').cast(IntegerType())).cast(TimestampType())) \\\n",
    "                                 .drop(\"trans_ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------+----------+----------------+--------------+--------------+------------+----------+------------+---------------------+----------------------+------------+-------------+--------------+\n",
      "|created_at|customer_id|transaction_id|session_id|product_metadata|payment_method|payment_status|promo_amount|promo_code|shipment_fee|shipment_location_lat|shipment_location_long|total_amount|clear_payment|event_trans_ts|\n",
      "+----------+-----------+--------------+----------+----------------+--------------+--------------+------------+----------+------------+---------------------+----------------------+------------+-------------+--------------+\n",
      "+----------+-----------+--------------+----------+----------------+--------------+--------------+------------+----------+------------+---------------------+----------------------+------------+-------------+--------------+\n",
      "\n",
      "+-----------------------+-----------+------------------------------------+------------------------------------+--------------------+--------------+--------------+------------+----------+------------+---------------------+----------------------+------------+-------------+-------------------+\n",
      "|created_at             |customer_id|transaction_id                      |session_id                          |product_metadata    |payment_method|payment_status|promo_amount|promo_code|shipment_fee|shipment_location_lat|shipment_location_long|total_amount|clear_payment|event_trans_ts     |\n",
      "+-----------------------+-----------+------------------------------------+------------------------------------+--------------------+--------------+--------------+------------+----------+------------+---------------------+----------------------+------------+-------------+-------------------+\n",
      "|2024-05-10 09:15:28.875|13812      |943ed375-4904-41b0-96de-640936f214d5|21c8b007-23cf-4803-b701-acdb2a64b07a|[{27513, 1, 175794}]|OVO           |Success       |0.0         |          |10000.0     |-8.23018519612676    |113.788942159051      |185794      |1            |2024-10-13 22:20:25|\n",
      "|2024-05-10 09:16:48.729|30992      |2b7ec867-8b73-47ac-89f0-e2d21bf7530a|dfff8f26-a55a-467e-a0ad-b64cbaefe77b|[{45765, 1, 460462}]|Credit Card   |Success       |0.0         |          |10000.0     |-6.92156442103678    |111.042779199557      |470462      |1            |2024-10-13 22:20:25|\n",
      "|2024-05-10 09:17:18.485|78811      |5e2b21f8-7a82-4394-b458-386c156fcc27|c5e9d527-3e4f-4980-ac45-fdb50880d704|[{16758, 1, 230714}]|Credit Card   |Success       |0.0         |          |5000.0      |0.481403778518175    |109.094886908601      |235714      |1            |2024-10-13 22:20:25|\n",
      "|2024-05-10 09:21:03.393|45205      |f4c43763-c09b-450d-aa81-28d54a3e6ad7|3addc7cf-7383-4e78-bbee-645604aff0c6|[{26329, 2, 215358}]|OVO           |Success       |3457.0      |BUYMORE   |15000.0     |-6.14056745562672    |106.731883141905      |740958      |1            |2024-10-13 22:20:25|\n",
      "|2024-05-10 09:21:12.223|55789      |41d56401-e394-45ac-9b8d-ab52ff61643d|e55f5d38-a777-497b-871f-fde9cb3b5348|[{57566, 1, 264137}]|Debit Card    |Success       |0.0         |          |15000.0     |-6.48634859623455    |107.982499453862      |279137      |1            |2024-10-13 22:20:25|\n",
      "+-----------------------+-----------+------------------------------------+------------------------------------+--------------------+--------------+--------------+------------+----------+------------+---------------------+----------------------+------------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the stream for transaction after converting data type\n",
    "trans_query = df_transactions \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(foreach_batch_function) \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\tAggregate the streaming data frames and create features you used in your assignment 2A model.  \n",
    "(note: customer ID has already been included in the stream.) Then, join the static data frames with the streaming data frame as our final data for prediction. Perform data type/column conversion according to your ML model and print out the Schema. (Again, you can reuse code from A2A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, min, max, hour, udf\n",
    "from pyspark.sql.types import StringType, TimestampType, IntegerType, DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the action levels\n",
    "L1_actions = {\"AP\", \"ATC\", \"CO\"}\n",
    "L2_actions = {\"VC\", \"VP\", \"VI\", \"SER\"}\n",
    "L3_actions = {\"SCR\", \"HP\", \"CL\"}\n",
    "\n",
    "# Define the UDF to categorize actions into each level\n",
    "def categorize_action(event_type):\n",
    "    if event_type in L1_actions:\n",
    "        return \"L1\"\n",
    "    elif event_type in L2_actions:\n",
    "        return \"L2\"\n",
    "    elif event_type in L3_actions:\n",
    "        return \"L3\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "categorize_action_udf = udf(categorize_action, StringType())\n",
    "\n",
    "# Register UDF for time of day categorization\n",
    "def time_of_day(hour):\n",
    "    if 6 <= hour < 12:\n",
    "        return 'morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'afternoon'\n",
    "    elif 18 <= hour < 24:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "time_of_day_udf = udf(time_of_day, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_schema_2A = StructType([\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True), \n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"product_metadata\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"payment_status\", StringType(), True),\n",
    "    StructField(\"promo_amount\", FloatType(), True),\n",
    "    StructField(\"promo_code\", StringType(), True),\n",
    "    StructField(\"shipment_fee\", FloatType(), True),\n",
    "    StructField(\"shipment_location_lat\", FloatType(), True),\n",
    "    StructField(\"shipment_location_long\", FloatType(), True),\n",
    "    StructField(\"total_amount\", FloatType(), True),\n",
    "    StructField(\"clear_payment\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "customer_trans_LTV_2A = spark.read.csv(\"transactions.csv\", schema=transaction_schema_2A, header = True)\n",
    "customer_trans_agg_2A = customer_trans_LTV_2A.groupBy(\"customer_id\").agg(\n",
    "    # Transaction-related aggregations\n",
    "    count(\"transaction_id\").alias(\"num_trans\"),\n",
    "    \n",
    "    # Count successful and failed transactions based on payment_status\n",
    "    count(when(col(\"payment_status\") == \"Success\", True)).alias(\"num_success_trans\"),\n",
    "    count(when(col(\"payment_status\") == \"Fail\", True)).alias(\"num_failed_trans\")\n",
    ")\n",
    "\n",
    "\n",
    "current_year = F.year(F.current_date())\n",
    "# Calculate age (round to integer) and first_join_year in the same step\n",
    "df_customer = df_customer.withColumn(\"age\", F.round(current_year - F.year(\"birthdate\"))) \\\n",
    "                         .withColumn(\"first_join_year\", F.year(\"first_join_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-----------------+----------------+\n",
      "|customer_id|num_trans|num_success_trans|num_failed_trans|\n",
      "+-----------+---------+-----------------+----------------+\n",
      "|      95519|       52|               42|              10|\n",
      "|      28426|       73|               62|              11|\n",
      "|      62481|       33|               27|               6|\n",
      "|      27108|       39|               28|              11|\n",
      "|      51244|      102|               74|              28|\n",
      "+-----------+---------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_trans_agg_2A.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = (\n",
    "    df_browsing_behaviour_watermark\n",
    "    # Categorize action levels and time of day\n",
    "    .withColumn(\"action_level\", categorize_action_udf(df_browsing_behaviour[\"event_type\"]))\n",
    "    .withColumn(\"event_hour\", hour(col(\"event_time\")))\n",
    "    .withColumn(\"time_of_day\", time_of_day_udf(col(\"event_hour\")))\n",
    "    .withColumn(\"event_hour\", hour(col(\"event_time\"))) \n",
    "    # Group by window and session_id and aggregate counts for each action level\n",
    "    .groupBy(F.window(\"event_ts\", \"2 minutes\", \"10 seconds\"), \"session_id\")\n",
    "    .agg(\n",
    "        F.count(F.when(F.col(\"action_level\") == \"L1\", 1)).alias(\"L1_count\"),\n",
    "        F.count(F.when(F.col(\"action_level\") == \"L2\", 1)).alias(\"L2_count\"),\n",
    "        F.count(F.when(F.col(\"action_level\") == \"L3\", 1)).alias(\"L3_count\"),\n",
    "        min(\"event_hour\").alias(\"min_hour\"),\n",
    "        max(\"event_hour\").alias(\"max_hour\")\n",
    "    )\n",
    "\n",
    "    # Add total_actions and ratios for L1 and L2\n",
    "    .withColumn(\"total_actions\", col(\"L1_count\") + col(\"L2_count\") + col(\"L3_count\"))\n",
    "    .withColumn(\"L1_ratio\", (col(\"L1_count\") / col(\"total_actions\")) * 100)\n",
    "    .withColumn(\"L2_ratio\", (col(\"L2_count\") / col(\"total_actions\")) * 100)\n",
    "    .drop(\"total_actions\")\n",
    "    .withColumn(\"medium_hour\", (col(\"min_hour\") + col(\"max_hour\")) / 2)\n",
    "    .withColumn(\"time_of_day_medium\", time_of_day_udf(col(\"medium_hour\")))\n",
    "    .join(\n",
    "        df_transactions.select(\"session_id\", \"payment_status\",\"promo_code\", \"customer_id\", \"product_metadata\",\"total_amount\", \"promo_amount\", \"shipment_fee\", \"shipment_location_lat\", \"shipment_location_long\",\"payment_method\"),\n",
    "        on=\"session_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .join(\n",
    "        df_customer.select(\"customer_id\", \"gender\",\"first_join_year\", \"age\"),\n",
    "        on=\"customer_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .join(customer_trans_agg_2A, on=\"customer_id\", how=\"left\")\n",
    "    .drop(\"min_hour\", \"max_hour\", \"medium_hour\")\n",
    "    .na.drop()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: float (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- L1_count: long (nullable = false)\n",
      " |-- L2_count: long (nullable = false)\n",
      " |-- L3_count: long (nullable = false)\n",
      " |-- L1_ratio: double (nullable = true)\n",
      " |-- L2_ratio: double (nullable = true)\n",
      " |-- time_of_day_medium: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- promo_code: string (nullable = true)\n",
      " |-- product_metadata: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- product_id: integer (nullable = true)\n",
      " |    |    |-- quantity: integer (nullable = true)\n",
      " |    |    |-- item_price: integer (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      " |-- promo_amount: float (nullable = true)\n",
      " |-- shipment_fee: float (nullable = true)\n",
      " |-- shipment_location_lat: float (nullable = true)\n",
      " |-- shipment_location_long: float (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- first_join_year: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- num_trans: long (nullable = true)\n",
      " |-- num_success_trans: long (nullable = true)\n",
      " |-- num_failed_trans: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define a dictionary with the required column data types\n",
    "required_dtypes = {\n",
    "    \"customer_id\": \"float\",\n",
    "    \"session_id\": \"string\",\n",
    "    \"payment_method\": \"string\", # 15\n",
    "    \"payment_status\": \"string\",\n",
    "    \"promo_amount\": \"float\", # 2\n",
    "    \"promo_code\": \"string\",\n",
    "    \"shipment_fee\": \"float\", # 3\n",
    "    \"shipment_location_lat\": \"float\", # 4\n",
    "    \"shipment_location_long\": \"float\", # 5\n",
    "    \"total_amount\": \"float\", # 1\n",
    "    \"L1_count\": \"long\", # 8\n",
    "    \"L2_count\": \"long\", # 9\n",
    "    \"L3_count\": \"long\", # 10\n",
    "    \"L1_ratio\": \"double\", # 11\n",
    "    \"L2_ratio\": \"double\", # 12\n",
    "    \"time_of_day_medium\": \"string\", # 14\n",
    "    \"gender\": \"string\",\n",
    "    \"age\": \"int\", # 6\n",
    "    \"first_join_year\": \"int\", # 7\n",
    "    \"num_trans\":\"long\", # 13\n",
    "    \"num_success_trans\": \"long\",\n",
    "    \"num_failed_trans\": \"long\"\n",
    "} \n",
    "# Cast each column to the correct data type\n",
    "for col_name, dtype in required_dtypes.items():\n",
    "    feature_df = feature_df.withColumn(col_name, col(col_name).cast(dtype))\n",
    "\n",
    "# Display the schema to verify the data types\n",
    "feature_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "|customer_id|session_id|window|L1_count|L2_count|L3_count|L1_ratio|L2_ratio|time_of_day_medium|payment_status|promo_code|product_metadata|total_amount|promo_amount|shipment_fee|shipment_location_lat|shipment_location_long|payment_method|gender|first_join_year|age|num_trans|num_success_trans|num_failed_trans|\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "|customer_id|session_id|window|L1_count|L2_count|L3_count|L1_ratio|L2_ratio|time_of_day_medium|payment_status|promo_code|product_metadata|total_amount|promo_amount|shipment_fee|shipment_location_lat|shipment_location_long|payment_method|gender|first_join_year|age|num_trans|num_success_trans|num_failed_trans|\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "|customer_id|session_id|window|L1_count|L2_count|L3_count|L1_ratio|L2_ratio|time_of_day_medium|payment_status|promo_code|product_metadata|total_amount|promo_amount|shipment_fee|shipment_location_lat|shipment_location_long|payment_method|gender|first_join_year|age|num_trans|num_success_trans|num_failed_trans|\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "|customer_id|session_id|window|L1_count|L2_count|L3_count|L1_ratio|L2_ratio|time_of_day_medium|payment_status|promo_code|product_metadata|total_amount|promo_amount|shipment_fee|shipment_location_lat|shipment_location_long|payment_method|gender|first_join_year|age|num_trans|num_success_trans|num_failed_trans|\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "|customer_id|session_id|window|L1_count|L2_count|L3_count|L1_ratio|L2_ratio|time_of_day_medium|payment_status|promo_code|product_metadata|total_amount|promo_amount|shipment_fee|shipment_location_lat|shipment_location_long|payment_method|gender|first_join_year|age|num_trans|num_success_trans|num_failed_trans|\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "|customer_id|session_id|window|L1_count|L2_count|L3_count|L1_ratio|L2_ratio|time_of_day_medium|payment_status|promo_code|product_metadata|total_amount|promo_amount|shipment_fee|shipment_location_lat|shipment_location_long|payment_method|gender|first_join_year|age|num_trans|num_success_trans|num_failed_trans|\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "|customer_id|session_id|window|L1_count|L2_count|L3_count|L1_ratio|L2_ratio|time_of_day_medium|payment_status|promo_code|product_metadata|total_amount|promo_amount|shipment_fee|shipment_location_lat|shipment_location_long|payment_method|gender|first_join_year|age|num_trans|num_success_trans|num_failed_trans|\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "+-----------+----------+------+--------+--------+--------+--------+--------+------------------+--------------+----------+----------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "\n",
      "+-----------+------------------------------------+------------------------------------------+--------+--------+--------+--------+--------+------------------+--------------+----------+--------------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "|customer_id|session_id                          |window                                    |L1_count|L2_count|L3_count|L1_ratio|L2_ratio|time_of_day_medium|payment_status|promo_code|product_metadata    |total_amount|promo_amount|shipment_fee|shipment_location_lat|shipment_location_long|payment_method|gender|first_join_year|age|num_trans|num_success_trans|num_failed_trans|\n",
      "+-----------+------------------------------------+------------------------------------------+--------+--------+--------+--------+--------+------------------+--------------+----------+--------------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "|75149.0    |2eaa3fb3-9209-4f3a-91b2-c563e633609c|{2024-10-13 22:19:00, 2024-10-13 22:21:00}|0       |1       |1       |0.0     |50.0    |night             |Success       |AZ2022    |[{59400, 1, 262501}]|257110.0    |5391.0      |0.0         |-1.0634433           |114.160324            |OVO           |F     |2021           |32 |12       |9                |3               |\n",
      "|75149.0    |2eaa3fb3-9209-4f3a-91b2-c563e633609c|{2024-10-13 22:19:10, 2024-10-13 22:21:10}|0       |1       |1       |0.0     |50.0    |night             |Success       |AZ2022    |[{59400, 1, 262501}]|257110.0    |5391.0      |0.0         |-1.0634433           |114.160324            |OVO           |F     |2021           |32 |12       |9                |3               |\n",
      "|75149.0    |2eaa3fb3-9209-4f3a-91b2-c563e633609c|{2024-10-13 22:18:50, 2024-10-13 22:20:50}|0       |1       |0       |0.0     |100.0   |night             |Success       |AZ2022    |[{59400, 1, 262501}]|257110.0    |5391.0      |0.0         |-1.0634433           |114.160324            |OVO           |F     |2021           |32 |12       |9                |3               |\n",
      "|40900.0    |d776328a-30fc-453b-b059-3e5c1c83cd47|{2024-10-13 22:19:00, 2024-10-13 22:21:00}|2       |2       |1       |40.0    |40.0    |night             |Fail          |AZ2022    |[{31311, 5, 468115}]|2339583.0   |10992.0     |10000.0     |-8.031628            |110.49691             |Debit Card    |M     |2020           |34 |9        |5                |4               |\n",
      "|40900.0    |d776328a-30fc-453b-b059-3e5c1c83cd47|{2024-10-13 22:19:10, 2024-10-13 22:21:10}|3       |3       |2       |37.5    |37.5    |night             |Fail          |AZ2022    |[{31311, 5, 468115}]|2339583.0   |10992.0     |10000.0     |-8.031628            |110.49691             |Debit Card    |M     |2020           |34 |9        |5                |4               |\n",
      "+-----------+------------------------------------+------------------------------------------+--------+--------+--------+--------+--------+------------------+--------------+----------+--------------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+------------------------------------+------------------------------------------+--------+--------+--------+--------+-----------------+------------------+--------------+----------+--------------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "|customer_id|session_id                          |window                                    |L1_count|L2_count|L3_count|L1_ratio|L2_ratio         |time_of_day_medium|payment_status|promo_code|product_metadata    |total_amount|promo_amount|shipment_fee|shipment_location_lat|shipment_location_long|payment_method|gender|first_join_year|age|num_trans|num_success_trans|num_failed_trans|\n",
      "+-----------+------------------------------------+------------------------------------------+--------+--------+--------+--------+-----------------+------------------+--------------+----------+--------------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "|4519.0     |2c33aa4d-e7fa-46dd-8458-88918911aad4|{2024-10-13 22:19:20, 2024-10-13 22:21:20}|0       |1       |0       |0.0     |100.0            |afternoon         |Success       |LIBURDONG |[{19482, 1, 196648}]|193776.0    |2872.0      |0.0         |-6.137211            |106.881714            |OVO           |F     |2019           |22 |11       |7                |4               |\n",
      "|4519.0     |2c33aa4d-e7fa-46dd-8458-88918911aad4|{2024-10-13 22:19:30, 2024-10-13 22:21:30}|0       |1       |0       |0.0     |100.0            |afternoon         |Success       |LIBURDONG |[{19482, 1, 196648}]|193776.0    |2872.0      |0.0         |-6.137211            |106.881714            |OVO           |F     |2019           |22 |11       |7                |4               |\n",
      "|6397.0     |efc2c0a1-b6f9-48a5-b161-f96bf8500143|{2024-10-13 22:19:30, 2024-10-13 22:21:30}|1       |0       |0       |100.0   |0.0              |afternoon         |Fail          |          |[{9207, 1, 171623}] |562907.0    |0.0         |10000.0     |0.85002714           |123.56083             |Credit Card   |F     |2017           |28 |60       |41               |19              |\n",
      "|65220.0    |6515636a-0b38-4426-89d6-95b5e05fb6cb|{2024-10-13 22:19:30, 2024-10-13 22:21:30}|0       |0       |1       |0.0     |0.0              |evening           |Fail          |          |[{52114, 1, 234771}]|244771.0    |0.0         |10000.0     |-2.639743            |113.00579             |OVO           |F     |2017           |22 |129      |93               |36              |\n",
      "|75149.0    |2eaa3fb3-9209-4f3a-91b2-c563e633609c|{2024-10-13 22:19:20, 2024-10-13 22:21:20}|0       |2       |1       |0.0     |66.66666666666666|morning           |Success       |AZ2022    |[{59400, 1, 262501}]|257110.0    |5391.0      |0.0         |-1.0634433           |114.160324            |OVO           |F     |2021           |32 |12       |9                |3               |\n",
      "+-----------+------------------------------------+------------------------------------------+--------+--------+--------+--------+-----------------+------------------+--------------+----------+--------------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_session = feature_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(foreach_batch_function) \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\tThe company is interested in the number of potential frauds as they happen and the products in customers’ shopping carts (so that they can plan their stock level ahead.) Load your ML model, and use the model to predict/process each browsing session/transaction as follows:  \n",
    "a)\tEvery 10 seconds, show the total number of potential frauds (prediction = 1) in the last 2 minutes, and persist the raw data (see 7a).  \n",
    "b)\tEvery 30 seconds, find the top 20 products (order by quantity descending) in the last 30 seconds, show product ID, name and total quantity. We only need the non-fraud transactions (prediction=0) by extracting customer shopping cart details (sum of all items of ADD_TO_CART(ATC) events from browsing behaviour, you can also extract it from transactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel  # Use PipelineModel if the model is part of a pipeline\n",
    "\n",
    "\n",
    "# Path where the model was saved (relative to the current directory)\n",
    "model_path = \"best_model\"  # This is your model path\n",
    "\n",
    "# Load the model using PipelineModel or just a generic model\n",
    "try:\n",
    "    loaded_model = PipelineModel.load(model_path)  \n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions successfully. Below is schema of prediction\n",
      "root\n",
      " |-- customer_id: float (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- L1_count: long (nullable = false)\n",
      " |-- L2_count: long (nullable = false)\n",
      " |-- L3_count: long (nullable = false)\n",
      " |-- L1_ratio: double (nullable = true)\n",
      " |-- L2_ratio: double (nullable = true)\n",
      " |-- time_of_day_medium: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- promo_code: string (nullable = true)\n",
      " |-- product_metadata: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- product_id: integer (nullable = true)\n",
      " |    |    |-- quantity: integer (nullable = true)\n",
      " |    |    |-- item_price: integer (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      " |-- promo_amount: float (nullable = true)\n",
      " |-- shipment_fee: float (nullable = true)\n",
      " |-- shipment_location_lat: float (nullable = true)\n",
      " |-- shipment_location_long: float (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- first_join_year: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- num_trans: long (nullable = true)\n",
      " |-- num_success_trans: long (nullable = true)\n",
      " |-- num_failed_trans: long (nullable = true)\n",
      " |-- time_of_day_medium_index: double (nullable = false)\n",
      " |-- payment_method_index: double (nullable = false)\n",
      " |-- time_of_day_medium_vec: vector (nullable = true)\n",
      " |-- payment_method_vec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the loaded model to make predictions\n",
    "try:\n",
    "    predictions = loaded_model.transform(feature_df)  # Use your DataFrame here\n",
    "    print(\"Predictions successfully. Below is schema of prediction\")\n",
    "    predictions.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted parquet folder and created checkpoint path successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Define the path to the parquet directory\n",
    "    parquet_directory_path = \"./parquet\"\n",
    "\n",
    "    # Remove the entire parquet directory if it exists\n",
    "    if os.path.exists(parquet_directory_path):\n",
    "        shutil.rmtree(parquet_directory_path)\n",
    "\n",
    "    # Checkpoint directory path\n",
    "    checkpoint_path = \"./parquet/predictions/checkpoint\"\n",
    "\n",
    "    print(\"Deleted parquet folder and created checkpoint path successfully.\")  # Fixed closing parenthesis\n",
    "\n",
    "except Exception as e:  # Catch all exceptions and assign to variable e\n",
    "    print(f\"An error occurred: {e}\")  # Print the specific error message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_stream = predictions \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"path\", \"./parquet/predictions\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraud_stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: float (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- L1_count: long (nullable = true)\n",
      " |-- L2_count: long (nullable = true)\n",
      " |-- L3_count: long (nullable = true)\n",
      " |-- L1_ratio: double (nullable = true)\n",
      " |-- L2_ratio: double (nullable = true)\n",
      " |-- time_of_day_medium: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- promo_code: string (nullable = true)\n",
      " |-- product_metadata: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- product_id: integer (nullable = true)\n",
      " |    |    |-- quantity: integer (nullable = true)\n",
      " |    |    |-- item_price: integer (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      " |-- promo_amount: float (nullable = true)\n",
      " |-- shipment_fee: float (nullable = true)\n",
      " |-- shipment_location_lat: float (nullable = true)\n",
      " |-- shipment_location_long: float (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- first_join_year: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- num_trans: long (nullable = true)\n",
      " |-- num_success_trans: long (nullable = true)\n",
      " |-- num_failed_trans: long (nullable = true)\n",
      " |-- time_of_day_medium_index: double (nullable = true)\n",
      " |-- payment_method_index: double (nullable = true)\n",
      " |-- time_of_day_medium_vec: vector (nullable = true)\n",
      " |-- payment_method_vec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_predictions_parquet = spark.readStream \\\n",
    "    .schema(predictions.schema) \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(\"./parquet/predictions\")\n",
    "\n",
    "read_predictions_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 0 - No records in this batch.\n",
      "Batch ID: 1 - No records in this batch.\n",
      "Batch ID: 2 - No records in this batch.\n",
      "Batch ID: 3 - No records in this batch.\n",
      "Batch ID: 4 - No records in this batch.\n",
      "Batch ID: 5 - No records in this batch.\n",
      "Batch 6 has 1853 records with total potential frauds is: 5.\n",
      "Batch 7 has 14813 records with total potential frauds is: 40.\n",
      "Batch 8 has 14272 records with total potential frauds is: 35.\n"
     ]
    }
   ],
   "source": [
    "def print_batch_count(batch_df, batch_id):\n",
    "    # Check if the batch DataFrame is empty\n",
    "    if batch_df.isEmpty():\n",
    "        print(f\"Batch ID: {batch_id} - No records in this batch.\")\n",
    "    else:\n",
    "        count = batch_df.count()\n",
    "        fraud_count = batch_df.filter(col(\"prediction\") == 1).count()\n",
    "        print(f\"Batch {batch_id} has {count} records with total potential frauds is: {fraud_count}.\")\n",
    "\n",
    "    \n",
    "read_predictions_parquet_query = read_predictions_parquet \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .foreachBatch(print_batch_count) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_predictions_parquet_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- total_quantity: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#6b\n",
    "# Assuming read_predictions_parquet already has the correct window structure\n",
    "non_fraud_products_df = (\n",
    "    read_predictions_parquet\n",
    "    .filter(F.col(\"prediction\") == 0)  # Filter for non-fraud transactions\n",
    "    .withColumn(\"exploded_product_data\", F.explode(\"product_metadata\"))  # Explode product metadata\n",
    "    .select(\n",
    "        F.col(\"exploded_product_data.product_id\").alias(\"product_id\"),\n",
    "        F.col(\"exploded_product_data.quantity\").alias(\"quantity\"),\n",
    "        F.col(\"window.start\").alias(\"window_start\")  # Add the window start for clarity\n",
    "    )\n",
    "    .join(\n",
    "        df_product.select(F.col(\"id\"), F.col(\"productDisplayName\").alias(\"product_name\")),\n",
    "        F.col(\"product_id\") == df_product.id\n",
    "    )\n",
    "    .drop(\"id\")\n",
    "    .withWatermark(\"window_start\", \"2 minutes\")  # Closing parenthesis added here\n",
    "    .groupBy(\n",
    "        F.window(\"window_start\", \"30 seconds\"),  # Group by the window.start for 30 seconds\n",
    "        \"product_id\",\n",
    "        \"product_name\"\n",
    "    )\n",
    "    .agg(F.sum(\"quantity\").alias(\"total_quantity\"))  # Sum the quantities\n",
    ")\n",
    "# Print the schema of the resulting DataFrame\n",
    "non_fraud_products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "def print_top_product(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        print(f\"Batch ID: {batch_id} - No records in this batch.\")\n",
    "    else:\n",
    "        batch_df \\\n",
    "        .orderBy(F.desc(\"total_quantity\")) \\\n",
    "        .select(\"product_id\", \"product_name\", \"total_quantity\") \\\n",
    "        .show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_fraud_products_query = non_fraud_products_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .foreachBatch(print_top_product) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_fraud_products_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\tWrite a Parquet file and save the following data frames (tip: you may look at part 3 and think about what columns to save):  \n",
    "a.\tPersist the raw data from 6a in parquet format. Every student may have different features/columns in their data frames depending on their model, at the bare minimum, we need some IDs to identify those frauds later on (transaction_id and/or session_id). After that, read the parquet file and show a few rows to verify it is saved correctly.  \n",
    "b.\tPersist the data from 6b in another parquet file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7a\n",
    "potential_fraud = read_predictions_parquet.filter(F.col(\"prediction\") == 1)\n",
    "potential_fraud_dectect_query = potential_fraud \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"path\", \"./parquet/potential_fraud_folder\") \\\n",
    "    .option(\"checkpointLocation\", \"./parquet/potential_fraud_folder/checkpoint\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------+--------+--------+-----------------+-----------------+------------------+--------------+----------+--------------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+------------------------+--------------------+----------------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "|customer_id|          session_id|              window|L1_count|L2_count|L3_count|         L1_ratio|         L2_ratio|time_of_day_medium|payment_status|promo_code|    product_metadata|total_amount|promo_amount|shipment_fee|shipment_location_lat|shipment_location_long|payment_method|gender|first_join_year|age|num_trans|num_success_trans|num_failed_trans|time_of_day_medium_index|payment_method_index|time_of_day_medium_vec|payment_method_vec|            features|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------------------+--------------------+--------+--------+--------+-----------------+-----------------+------------------+--------------+----------+--------------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+------------------------+--------------------+----------------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    65702.0|aa63d0c5-2d34-474...|{2024-10-13 22:26...|       7|       1|       1|77.77777777777779|11.11111111111111|         afternoon|       Success|          |[{25452, 1, 319876}]|   1945499.0|         0.0|     25000.0|            1.9733355|             116.38268|         Gopay|     F|           2016| 32|       86|               74|              12|                     1.0|                 1.0|         (3,[1],[1.0])|     (4,[1],[1.0])|[0.0,1.0,0.0,0.0,...|[-1.5118344140134...|[0.04636797606169...|       1.0|\n",
      "|    65702.0|aa63d0c5-2d34-474...|{2024-10-13 22:26...|       7|       1|       1|77.77777777777779|11.11111111111111|         afternoon|       Success|          |[{25452, 1, 319876}]|   1945499.0|         0.0|     25000.0|            1.9733355|             116.38268|         Gopay|     F|           2016| 32|       86|               74|              12|                     1.0|                 1.0|         (3,[1],[1.0])|     (4,[1],[1.0])|[0.0,1.0,0.0,0.0,...|[-1.5118344140134...|[0.04636797606169...|       1.0|\n",
      "|     5999.0|a1a1d87e-272b-4d3...|{2024-10-13 22:26...|      10|       0|       0|            100.0|              0.0|           morning|       Success|   BUYMORE|[{43817, 1, 141736}]|   6448465.0|      2665.0|     25000.0|            1.0557582|             113.59184|   Credit Card|     F|           2018| 31|      142|               98|              44|                     0.0|                 0.0|         (3,[0],[1.0])|     (4,[0],[1.0])|(20,[0,3,7,8,9,10...|[-1.6095964559808...|[0.03844981358385...|       1.0|\n",
      "|     5999.0|a1a1d87e-272b-4d3...|{2024-10-13 22:26...|       9|       0|       0|            100.0|              0.0|           morning|       Success|   BUYMORE|[{43817, 1, 141736}]|   6448465.0|      2665.0|     25000.0|            1.0557582|             113.59184|   Credit Card|     F|           2018| 31|      142|               98|              44|                     0.0|                 0.0|         (3,[0],[1.0])|     (4,[0],[1.0])|(20,[0,3,7,8,9,10...|[-1.6095964559808...|[0.03844981358385...|       1.0|\n",
      "|     5999.0|a1a1d87e-272b-4d3...|{2024-10-13 22:26...|      12|       1|       1|85.71428571428571|7.142857142857142|           morning|       Success|   BUYMORE|[{43817, 1, 141736}]|   6448465.0|      2665.0|     25000.0|            1.0557582|             113.59184|   Credit Card|     F|           2018| 31|      142|               98|              44|                     0.0|                 0.0|         (3,[0],[1.0])|     (4,[0],[1.0])|[1.0,0.0,0.0,1.0,...|[-1.6095964559808...|[0.03844981358385...|       1.0|\n",
      "+-----------+--------------------+--------------------+--------+--------+--------+-----------------+-----------------+------------------+--------------+----------+--------------------+------------+------------+------------+---------------------+----------------------+--------------+------+---------------+---+---------+-----------------+----------------+------------------------+--------------------+----------------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    potential_fraud_7a_df = spark.read.parquet(\"./parquet/potential_fraud_folder\")\n",
    "    potential_fraud_7a_df.show(5)\n",
    "except AnalysisException as e:\n",
    "    print(\"Please re-run this after few mins.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential_fraud_dectect_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7b\n",
    "top_product_7b_query = non_fraud_products_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"path\", \"./parquet/top_product_non_fraud\") \\\n",
    "    .option(\"checkpointLocation\", \"./parquet/top_product_non_fraud/checkpoint\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------+\n",
      "|              window|product_id|        product_name|total_quantity|\n",
      "+--------------------+----------+--------------------+--------------+\n",
      "|{2024-10-13 22:25...|      2978|Puma Women's Punk...|             9|\n",
      "|{2024-10-13 22:25...|     34136|Turtle Men Leathe...|             2|\n",
      "|{2024-10-13 22:24...|     13517|Chimp Men Dracull...|             4|\n",
      "|{2024-10-13 22:25...|     49152|Deborah Extra Lip...|            15|\n",
      "|{2024-10-13 22:26...|     20234|Wrangler Women Ba...|             3|\n",
      "+--------------------+----------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    top_product_non_fraud_df = spark.read.parquet(\"./parquet/top_product_non_fraud\")\n",
    "    top_product_non_fraud_df.show(5)\n",
    "except AnalysisException as e:\n",
    "    print(\"Please re-run this after few mins.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_product_7b_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\tRead the two parquet files from task 7 as data streams and send to Kafka topics with appropriate names.\n",
    "(Note: You shall read the parquet files as a streaming data frame and send messages to the Kafka topic when new data appears in the parquet file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 1\n",
    "potential_fraud_streamline = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .schema(potential_fraud.schema) \\\n",
    "    .option(\"path\", \"./parquet/potential_fraud_folder\") \\\n",
    "    .load()\n",
    "\n",
    "# Sending messages to Kafka topic 'potential_fraud_topic'\n",
    "potential_fraud_query = potential_fraud_streamline \\\n",
    "    .selectExpr(\"to_json(struct(*)) as value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"potential_fraud_topic\") \\\n",
    "    .option(\"checkpointLocation\", \"./parquet/potential_fraud_folder/kafka_checkpoint\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_fraud_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 2\n",
    "top_product_non_fraud_streamline = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"./parquet/top_product_non_fraud\") \\\n",
    "    .schema(non_fraud_products_df.schema) \\\n",
    "    .load()\n",
    "\n",
    "# Sending messages to Kafka topic 'top_product_fraud_topic'\n",
    "top_product_non_fraud_query = top_product_non_fraud_streamline \\\n",
    "    .selectExpr(\"to_json(struct(*)) as value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"top_product_non_fraud_topic\") \\\n",
    "    .option(\"checkpointLocation\", \"./parquet/top_product_non_fraud/kafka_checkpoint\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_product_non_fraud_query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
