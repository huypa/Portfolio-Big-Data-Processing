{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Producing the data\n",
    "In this task, we will implement Apache Kafka producers to simulate real-time data streaming. Spark is not allowed in this part since it’s simulating a streaming data source.  \n",
    "\n",
    "1.\tYour program should send one batch of browsing behaviour data every 5 seconds. One batch consists of a random 500-1000 rows from the browsing behaviour dataset. The CSV shouldn’t be loaded to memory at once to conserve memory (i.e. Read row as needed). Keep track of the start and end event_time. (You can assume the dataset is sorted by event_time.)  \n",
    "2.\tAdd an integer column named ‘ts’ for each row, a Unix timestamp in seconds since the epoch. Spead your batch out evenly for 5 seconds.  \n",
    "a.\tFor example, if you send a batch of 600 records at 2023-09-01 00:00:00 (ISO format: YYYY-MM-DD HH:MM:SS) -> (ts = 1693526400):  \n",
    "-\tRecord 1-120: ts = 1693526400   \n",
    "-\tRecord 121-240: ts = 1693526401   \n",
    "-\tRecord 241-360: ts = 1693526402  \n",
    "-\t….  \n",
    "3.\tRead the transactions between the start and end event_time in 1.1 every 5 seconds (the same frequency as browsing behaviour) and create a batch.  \n",
    "4.\tSend your two batches from 1.1 and 1.3 to Kafka topics with an appropriate name.  \n",
    "Note 1: In 1.1, “random 500-1000” means the number of rows is random, and the data file is still read sequentially.  \n",
    "Note 2: All the data except for the ‘ts’ column should be sent in the original String type without changing to any other type. This is because we are simulating a streaming access log and need to reduce the required processing at the source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "from kafka3 import KafkaProducer\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka Producer Configuration\n",
    "# producer = KafkaProducer(bootstrap_servers='localhost:9092', \n",
    "#                          value_serializer=lambda v: str(v).encode('utf-8'))\n",
    "hostip = \"kafka\"\n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=[f'{hostip}:9092'], api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_in_batches(file_path):\n",
    "    batch_size = random.randint(500, 1000)\n",
    "    \n",
    "    # Open the CSV file\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Read the header\n",
    "\n",
    "        while True:\n",
    "            # Read a batch of rows\n",
    "            rows = []\n",
    "            for _ in range(batch_size):\n",
    "                try:\n",
    "                    row = next(reader)  # Read the next row from the reader\n",
    "                    rows.append(row)\n",
    "                except StopIteration:\n",
    "                    break  # End of file\n",
    "            \n",
    "            if not rows:\n",
    "                break  # No more rows\n",
    "            \n",
    "            yield header, rows  # Yield header and rows\n",
    "# Function to read transactions in a time range\n",
    "def read_transactions_in_timerange(trans_path, start_time, end_time):\n",
    "    with open(trans_path, 'r') as file:\n",
    "        # Use csv.reader to handle the CSV format correctly\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Read the header\n",
    "#         print(header, '\\n')\n",
    "\n",
    "        for row in reader:\n",
    "            event_time = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S.%f')  # Adjust for milliseconds\n",
    "            if start_time <= event_time <= end_time:\n",
    "#                 print(row, '\\n')  # Print the raw row for verification\n",
    "                yield dict(zip(header, row))  # Yield the transaction as a dictionary\n",
    "                \n",
    "# Function to produce batches of data to Kafka\n",
    "def produce_batches(producer, browsing_path, browsing_topic_name, trans_path, trans_topic_name):\n",
    "    \n",
    "    batch_generator = read_data_in_batches(browsing_path)\n",
    "    \n",
    "    for header, rows in batch_generator:\n",
    "#         print('Start public batch:', '\\n')\n",
    "        \n",
    "        # Get the start and end event_time\n",
    "        start_time = datetime.strptime(rows[0][2].strip('\"'), '%Y-%m-%d %H:%M:%S.%f')  # Adjusted for milliseconds\n",
    "        end_time = datetime.strptime(rows[-1][2].strip('\"'), '%Y-%m-%d %H:%M:%S.%f')  # Adjusted for milliseconds\n",
    "\n",
    "        # Send records with timestamps\n",
    "        for i, row in enumerate(rows):\n",
    "            row_dict = dict(zip(header, row))  # Create a dictionary from the header and row\n",
    "            \n",
    "            # Calculate the timestamp for each record\n",
    "            current_time = int(datetime.now().timestamp())  # Current Unix timestamp\n",
    "            ts = current_time + (i // (len(rows) // 5)) \n",
    "            row_dict['ts'] = ts  # Add the timestamp\n",
    "            \n",
    "#             print(row_dict)  # Print for verification\n",
    "            producer.send(browsing_topic_name, value=json.dumps(row_dict).encode('utf-8'))  # Send to Kafka\n",
    "\n",
    "        # Read and send transactions in the specified time range\n",
    "        transactions = read_transactions_in_timerange(trans_path, start_time, end_time)\n",
    "        for transaction in transactions:\n",
    "            trans_current_time = int(datetime.now().timestamp())\n",
    "            transaction['trans_ts'] = trans_current_time\n",
    "#             print(transaction)  # Print for verification\n",
    "            producer.send(trans_topic_name, value=json.dumps(transaction).encode('utf-8'))  # Send to Kafka\n",
    "#         print('Finish batch', '\\n')\n",
    "        \n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    producer = connect_kafka_producer()\n",
    "    browsing_path = 'new_browsing_behaviour.csv'  # Path to your browsing behavior CSV file\n",
    "    trans_path = 'new_transactions.csv'  # Path to your transactions CSV file\n",
    "    browsing_topic_name = 'browsing_behaviour_topic'  # Kafka topic for browsing behavior\n",
    "    trans_topic_name = 'trans_topic'  # Kafka topic for transactions\n",
    "\n",
    "    while True:\n",
    "        produce_batches(producer, browsing_path, browsing_topic_name, trans_path, trans_topic_name)\n",
    "        time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
